{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matej/.cache/pypoetry/virtualenvs/ul-fri-nlp-course-project-iota-g_O3Pc_g-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# https://jmlr.org/papers/volume21/20-074/20-074.pdf\n",
    "model_name = \"Falconsai/text_summarization\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_book(path):\n",
    "    summarize_path = os.path.join(\"..\", \"summaries\", path)\n",
    "\n",
    "    if os.path.exists(summarize_path):\n",
    "        print(f\"{path} already summarized\")\n",
    "        return\n",
    "\n",
    "    with open(os.path.join(\"..\", \"books\", path), \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        chunk_size=512, chunk_overlap=0, tokenizer=summarizer.tokenizer\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"{path} split into {len(chunks)} chunks\")\n",
    "\n",
    "    with open(summarize_path, \"w\") as f:\n",
    "        for chunk in tqdm(chunks):\n",
    "            summary = summarizer(chunk)\n",
    "            f.write(summary[0][\"summary_text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp/Book 3 - Prisoner of Azkaban.txt already summarized\n",
      "hp/Book 4 - Goblet of Fire.txt already summarized\n",
      "hp/Book 2 - Chamber of Secrets.txt already summarized\n",
      "hp/Book 5 - Order of the Phoenix.txt already summarized\n",
      "hp/Book 7 - Deathly Hallows.txt already summarized\n",
      "hp/Book 6 - Half Blood Prince.txt already summarized\n",
      "hp/Book 1 - Philosophers Stone.txt already summarized\n",
      "asoif/Book 2 - A Clash of Kings.txt already summarized\n",
      "asoif/characters.txt already summarized\n",
      "asoif/Book 5 - A Dance With Dragons.txt already summarized\n",
      "asoif/Book 3 - A Storm of Swords.txt already summarized\n",
      "asoif/Book 1 - A Game of Thrones.txt already summarized\n",
      "asoif/Book 4 - A Feast for Crows.txt split into 1094 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1094 [00:15<29:03,  1.61s/it] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      " 40%|███▉      | 437/1094 [11:40<18:24,  1.68s/it]Your max_length is set to 200, but your input_length is only 197. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=98)\n",
      "100%|█████████▉| 1093/1094 [30:26<00:01,  1.84s/it]Your max_length is set to 200, but your input_length is only 176. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=88)\n",
      "100%|██████████| 1094/1094 [30:29<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for series in [\"hp\", \"asoif\"]:\n",
    "    for book in os.listdir(os.path.join(\"..\", \"books\", series)):\n",
    "        summarize_book(os.path.join(series, book))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ul-fri-nlp-course-project-iota-g_O3Pc_g-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
